{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training       28709\n",
       "PublicTest      3589\n",
       "PrivateTest     3589\n",
       "Name: Usage, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Usage\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D, Dropout, Conv2D,Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_col = data[\"pixels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = [],[],[],[],[],[]\n",
    "for index, row_value in data.iterrows():\n",
    "    value = row_value[\"pixels\"].split(\" \")\n",
    "    try:\n",
    "        if \"Training\" in row_value[\"Usage\"]:\n",
    "            X_train.append(np.array(value, 'float32'))\n",
    "            y_train.append(row_value[\"emotion\"])\n",
    "        elif \"PublicTest\" in row_value[\"Usage\"]:\n",
    "            X_test.append(np.array(value, \"float32\"))\n",
    "            y_test.append(row_value[\"emotion\"])\n",
    "        else:\n",
    "            X_val.append(np.array(value, \"float32\"))\n",
    "            y_val.append(row_value[\"emotion\"])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "X_val = np.array(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.,  80.,  82., ..., 106., 109.,  82.],\n",
       "       [151., 150., 147., ..., 193., 183., 184.],\n",
       "       [231., 212., 156., ...,  88., 110., 152.],\n",
       "       ...,\n",
       "       [ 74.,  81.,  87., ..., 188., 187., 187.],\n",
       "       [222., 227., 203., ..., 136., 136., 134.],\n",
       "       [195., 199., 205., ...,   6.,  15.,  38.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "img_row = 48\n",
    "img_col = 48\n",
    "num_label = 7\n",
    "\n",
    "from keras.utils import to_categorical, np_utils\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=num_label)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=num_label)\n",
    "y_val = np_utils.to_categorical(y_val, num_classes=num_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_val = X_val / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], img_row, img_col, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_row, img_col, 1)\n",
    "X_val =X_val.reshape(X_val.shape[0], img_row, img_col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "898/898 [==============================] - 179s 199ms/step - loss: 1.6910 - accuracy: 0.3222 - val_loss: 1.5142 - val_accuracy: 0.4257\n",
      "Epoch 2/20\n",
      "898/898 [==============================] - 180s 201ms/step - loss: 1.4486 - accuracy: 0.4433 - val_loss: 1.3516 - val_accuracy: 0.4770\n",
      "Epoch 3/20\n",
      "898/898 [==============================] - 179s 199ms/step - loss: 1.3289 - accuracy: 0.4920 - val_loss: 1.2810 - val_accuracy: 0.5138\n",
      "Epoch 4/20\n",
      "898/898 [==============================] - 183s 204ms/step - loss: 1.2627 - accuracy: 0.5182 - val_loss: 1.2219 - val_accuracy: 0.5366\n",
      "Epoch 5/20\n",
      "898/898 [==============================] - 190s 212ms/step - loss: 1.2083 - accuracy: 0.5405 - val_loss: 1.1798 - val_accuracy: 0.5522\n",
      "Epoch 6/20\n",
      "898/898 [==============================] - 187s 208ms/step - loss: 1.1623 - accuracy: 0.5585 - val_loss: 1.1516 - val_accuracy: 0.5631\n",
      "Epoch 7/20\n",
      "898/898 [==============================] - 181s 202ms/step - loss: 1.1237 - accuracy: 0.5727 - val_loss: 1.1427 - val_accuracy: 0.5637\n",
      "Epoch 8/20\n",
      "898/898 [==============================] - 183s 204ms/step - loss: 1.0882 - accuracy: 0.5879 - val_loss: 1.1602 - val_accuracy: 0.5561\n",
      "Epoch 9/20\n",
      "898/898 [==============================] - 184s 205ms/step - loss: 1.0518 - accuracy: 0.6048 - val_loss: 1.1519 - val_accuracy: 0.5678\n",
      "Epoch 10/20\n",
      "898/898 [==============================] - 184s 205ms/step - loss: 1.0201 - accuracy: 0.6186 - val_loss: 1.1292 - val_accuracy: 0.5787\n",
      "Epoch 11/20\n",
      "898/898 [==============================] - 184s 205ms/step - loss: 0.9989 - accuracy: 0.6218 - val_loss: 1.1824 - val_accuracy: 0.5631\n",
      "Epoch 12/20\n",
      "898/898 [==============================] - 193s 215ms/step - loss: 0.9582 - accuracy: 0.6391 - val_loss: 1.1373 - val_accuracy: 0.5756\n",
      "Epoch 13/20\n",
      "898/898 [==============================] - 196s 218ms/step - loss: 0.9359 - accuracy: 0.6475 - val_loss: 1.1514 - val_accuracy: 0.5745\n",
      "Epoch 14/20\n",
      "898/898 [==============================] - 200s 222ms/step - loss: 0.8968 - accuracy: 0.6631 - val_loss: 1.1549 - val_accuracy: 0.5776\n",
      "Epoch 15/20\n",
      "898/898 [==============================] - 190s 211ms/step - loss: 0.8867 - accuracy: 0.6682 - val_loss: 1.1532 - val_accuracy: 0.5790\n",
      "Epoch 16/20\n",
      "898/898 [==============================] - 183s 204ms/step - loss: 0.8531 - accuracy: 0.6784 - val_loss: 1.1787 - val_accuracy: 0.5653\n",
      "Epoch 17/20\n",
      "898/898 [==============================] - 187s 208ms/step - loss: 0.8301 - accuracy: 0.6899 - val_loss: 1.1482 - val_accuracy: 0.5832\n",
      "Epoch 18/20\n",
      "898/898 [==============================] - 188s 210ms/step - loss: 0.8069 - accuracy: 0.7014 - val_loss: 1.1892 - val_accuracy: 0.5756\n",
      "Epoch 19/20\n",
      "898/898 [==============================] - 184s 205ms/step - loss: 0.7837 - accuracy: 0.7096 - val_loss: 1.2074 - val_accuracy: 0.5743\n",
      "Epoch 20/20\n",
      "898/898 [==============================] - 182s 203ms/step - loss: 0.7694 - accuracy: 0.7129 - val_loss: 1.2157 - val_accuracy: 0.5790\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=X_train.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(num_label))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 5s 46ms/step - loss: 1.2482 - accuracy: 0.5712\n",
      "WARNING:tensorflow:From c:\\users\\it park\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: facecnn.model\\assets\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "\n",
    "model.save(\"facecnn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"facecnn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-88852b3ee1b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mroi_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgray_img\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mroi_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroi_gray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mimg_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroi_gray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mimg_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_pixels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mimg_pixels\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, test_img = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        img = cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray_img[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "        img_pixels /= 255.0\n",
    "        \n",
    "        predictions = model.predict(img_pixels)\n",
    "        max_index = np.argmax(predictions[0])\n",
    "        emotions = (\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\")\n",
    "        predict_emo = emotions[max_index]\n",
    "        cv2.putText(test_img, predict_emo, (int(x), int(y)), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "    resized_img = cv2.resize(test_img, (1000, 700))\n",
    "    cv2.imshow(\"Emotion Detect\", resized_img)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
